{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhR-ZUkwJrFn"
   },
   "source": [
    "## Problem Statement \n",
    "\n",
    "You need to build a model that is able to classify customer complaints based on the products/services. By doing so, you can segregate these tickets into their relevant categories and, therefore, help in the quick resolution of the issue.\n",
    "\n",
    "You will be doing topic modelling on the <b>.json</b> data provided by the company. Since this data is not labelled, you need to apply NMF to analyse patterns and classify tickets into the following five clusters based on their products/services:\n",
    "\n",
    "* Credit card / Prepaid card\n",
    "\n",
    "* Bank account services\n",
    "\n",
    "* Theft/Dispute reporting\n",
    "\n",
    "* Mortgages/loans\n",
    "\n",
    "* Others \n",
    "\n",
    "\n",
    "With the help of topic modelling, you will be able to map each ticket onto its respective department/category. You can then use this data to train any supervised model such as logistic regression, decision tree or random forest. Using this trained model, you can classify any new customer complaint support ticket into its relevant department."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcgXVNyaLUFS"
   },
   "source": [
    "## Pipelines that needs to be performed:\n",
    "\n",
    "You need to perform the following eight major tasks to complete the assignment:\n",
    "\n",
    "1.  Data loading\n",
    "\n",
    "2. Text preprocessing\n",
    "\n",
    "3. Exploratory data analysis (EDA)\n",
    "\n",
    "4. Feature extraction\n",
    "\n",
    "5. Topic modelling \n",
    "\n",
    "6. Model building using supervised learning\n",
    "\n",
    "7. Model training and evaluation\n",
    "\n",
    "8. Model inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JuLFIymAL58u"
   },
   "source": [
    "## Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We used Jarvis so kept all the below install/download command\n",
    "\n",
    "!pip install spacy\n",
    "!pip install plotly\n",
    "!pip install wordcloud\n",
    "\n",
    "import nltk, spacy\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O-Q9pqrcJrFr"
   },
   "outputs": [],
   "source": [
    "# importing all the libraries\n",
    "\n",
    "import json \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re, nltk, spacy, string\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from plotly.offline import plot\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from pprint import pprint\n",
    "\n",
    "# Suppressing Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtRLCsNVJrFt"
   },
   "source": [
    "## Loading the data\n",
    "\n",
    "The data is in JSON format and we need to convert it to a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "puVzIf_iJrFt"
   },
   "outputs": [],
   "source": [
    "# Opening JSON file.\n",
    "# Write the path to your data file and load it \n",
    "f = open('complaints-2021-05-14_08_16.json', 'r')\n",
    "\n",
    "# returns JSON object as a dictionary \n",
    "data = json.load(f)\n",
    "df = pd.json_normalize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xYpH-sAJrFu"
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lf8ufHH5JrFu"
   },
   "outputs": [],
   "source": [
    "# Inspect the dataframe to understand the given data\n",
    "\n",
    "print(\"Shape of dataframe:-\", df.shape)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dwcty-wmJrFw"
   },
   "outputs": [],
   "source": [
    "#print the column names\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment about renaming columns:\n",
    "- All the column name is having suffix as `_source` except the first 4 columns\n",
    "- First add above suffix to first 4 columns also\n",
    "- Then rename all the columns to remove suffux `_source`\n",
    "- Reduce the columns name `complaint_what_happened` to `complaints`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYCtKXD1JrFw"
   },
   "outputs": [],
   "source": [
    "#Assign new column names\n",
    "\n",
    "df.rename({'_index': '_source.index', '_type': '_source.type','_id': '_source.id', '_score': '_source.score'}, axis=1, inplace=True)\n",
    "df.rename(columns=lambda x: x.replace('_source.', ''), inplace=True)\n",
    "df.rename({'complaint_what_happened': 'complaints'}, axis=1, inplace=True)\n",
    "\n",
    "# print new column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grQUPFL5JrFx"
   },
   "outputs": [],
   "source": [
    "#Use regex to assign nan in place of blanks in the complaints column\n",
    "\n",
    "df['complaints'] = df['complaints'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jfxd8VSmJrFy"
   },
   "outputs": [],
   "source": [
    "#Remove all rows where complaints column is nan\n",
    "\n",
    "df.dropna(subset = [\"complaints\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shape of df after cleaning the dataset\n",
    "print(\"Shape of dataframe:-\", df.shape)\n",
    "\n",
    "# print the head also\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L944HZpsJrFy"
   },
   "source": [
    "## Prepare the text for topic modeling\n",
    "\n",
    "Once you have removed all the blank complaints, you need to:\n",
    "\n",
    "* Make the text lowercase\n",
    "* Remove text in square brackets\n",
    "* Remove punctuation\n",
    "* Remove words containing numbers\n",
    "\n",
    "\n",
    "Once you have done these cleaning operations you need to perform the following:\n",
    "* Lemmatize the texts\n",
    "* Use POS tags to get relevant words from the texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qm7SjjSkJrFz"
   },
   "outputs": [],
   "source": [
    "# Write your function here to clean the text and remove all the unnecessary elements.\n",
    "\n",
    "def clean_text(df):\n",
    "    \n",
    "    df['complaints'] = df['complaints'].apply(lambda x: x.lower()) # Make text in lower case\n",
    "    df['complaints'] = df['complaints'].apply(lambda x: x.replace(\"\\n\",\"\")) # Remove new line char\n",
    "    df['complaints'] = df['complaints'].replace(r'(\\w*\\d\\w*)', '', regex=True) # Remove word containing numbers\n",
    "    df['complaints'] = df['complaints'].replace(r'[^\\w\\s]', '', regex=True) # Remove punctuation\n",
    "    df['complaints'] = df['complaints'].replace(r'\\([^)]*\\)|\\{[^)]*\\}|\\[[^)]*\\]', '', regex=True) #Remove text in square bracket\n",
    "    \n",
    "    return df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the clean_text column to clean complaint column\n",
    "\n",
    "clean_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a complaint list from df.complaints column\n",
    "\n",
    "com_list = df.complaints.tolist()\n",
    "\n",
    "# Print couple of sentences\n",
    "com_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zgOu8t8HJrFz"
   },
   "outputs": [],
   "source": [
    "#Write your function to Lemmatize the texts\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "def preprocess(document):\n",
    "    'removes stopwords and lemmatizes the remainder of the sentence'\n",
    "\n",
    "    # tokenize into words\n",
    "    words = word_tokenize(document)\n",
    "    \n",
    "    words = [wordnet_lemmatizer.lemmatize(word, pos='n') for word in words if word not in stop_words]\n",
    "\n",
    "    # join words to make sentence\n",
    "    document = \" \".join(words)\n",
    "    \n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call preprocess function to create lemmatization list using complaint list\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemma_list = [preprocess(sentence) for sentence in com_list]\n",
    "\n",
    "# print couple of sentences from lemmatize list\n",
    "lemma_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uXnN7aa_JrF0"
   },
   "outputs": [],
   "source": [
    "#Create a dataframe('df_clean') that will have only the complaints and the lemmatized complaints \n",
    "\n",
    "df_clean = pd.DataFrame(list(zip(com_list, lemma_list)), columns =['complaint', 'lemma_Com'])\n",
    "\n",
    "# print head\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kk7fc4DuJrF1"
   },
   "outputs": [],
   "source": [
    "#Write your function to extract the POS tags (Keep only NN Pos tags)\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser','ner'])\n",
    "\n",
    "def get_POS_tags(document):\n",
    "    #pos_tags = \"\"\n",
    "    words = \"\"\n",
    "       \n",
    "    tagged_sentence = nlp(document)\n",
    "    \n",
    "    for token in tagged_sentence:\n",
    "        if token.pos_ == \"NOUN\":\n",
    "            words += token.text + \" \" \n",
    "    \n",
    "    return words\n",
    "    #return words,pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call get_POS_tags function to get the words with NOUN POS tags only.  Create a new list com_after_rem_pos.\n",
    "\n",
    "com_after_rem_pos = [get_POS_tags(sentence) for sentence in com_list]\n",
    "\n",
    "# Print first couple of lines\n",
    "com_after_rem_pos[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AjxfchvFJrF2"
   },
   "outputs": [],
   "source": [
    "#The clean dataframe should now contain the raw complaint, lemmatized complaint and the complaint after removing POS tags.\n",
    "\n",
    "# add column complaint after removing POS tags\n",
    "df_clean['com_after_rem_pos'] = com_after_rem_pos\n",
    "\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7Un1AElJrF2"
   },
   "source": [
    "## Exploratory data analysis to get familiar with the data.\n",
    "\n",
    "Write the code in this task to perform the following:\n",
    "\n",
    "*   Visualise the data according to the 'Complaint' character length\n",
    "*   Using a word cloud find the top 40 words by frequency among all the articles after processing the text\n",
    "*   Find the top unigrams,bigrams and trigrams by frequency among all the complaints after processing the text. ‘\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-zaqJF6JrF2"
   },
   "outputs": [],
   "source": [
    "# Write your code here to visualise the data according to the 'Complaint' character length\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "doc_lens = [len(d) for d in df_clean.complaint]\n",
    "plt.hist(doc_lens, bins = 50)\n",
    "plt.xlabel('complaint_bins')\n",
    "plt.ylabel('Character_length');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9jD_6SeJrF3"
   },
   "source": [
    "#### Find the top 40 words by frequency among all the articles after processing the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QcfdvtfZJrF3"
   },
   "outputs": [],
   "source": [
    "#Using a word cloud find the top 40 words by frequency among all the articles after processing the text\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud(stopwords=stop_words,max_words=40).generate(str(df_clean.com_after_rem_pos))\n",
    "\n",
    "print(wordcloud)\n",
    "plt.figure(figsize=(10,6))\n",
    "fig = plt.figure(1)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OkSmc3UaJrF4"
   },
   "outputs": [],
   "source": [
    "print(spacy.__version__)\n",
    "\n",
    "#Removing -PRON- from the text corpus\n",
    "#df_clean['Complaint_clean'] = df_clean['complaint_POS_removed'].str.replace('-PRON-', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Since we are using new version of spacy we don't need to perfom above step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DfCSbbmJrF4"
   },
   "source": [
    "#### Find the top unigrams,bigrams and trigrams by frequency among all the complaints after processing the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5mbk5DS5JrF4"
   },
   "outputs": [],
   "source": [
    "#Write your code here to find the top 30 unigram frequency among the complaints in the cleaned datafram(df_clean). \n",
    "\n",
    "# function to find the top n unigram\n",
    "def get_top_n_unigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(1, 1), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "      \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    \n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YX7fedm1JrF8"
   },
   "outputs": [],
   "source": [
    "# finding the top 30 unigrams\n",
    "\n",
    "common_words_uni = get_top_n_unigram(df_clean.com_after_rem_pos, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the top 10 words in the unigram frequency\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "df_uni = pd.DataFrame(common_words_uni[0:10], columns = ['unigram' , 'count'])\n",
    "fig = sns.barplot(y=df_uni['unigram'], x=df_uni['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aV7kD7w8JrF8"
   },
   "outputs": [],
   "source": [
    "#Write your code here to find the top 30 bigram frequency among the complaints in the cleaned datafram(df_clean). \n",
    "\n",
    "# function to find the top n bigram\n",
    "\n",
    "def get_top_n_bigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "       \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    \n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NPnMNIpyJrF9"
   },
   "outputs": [],
   "source": [
    "# finding the top 30 bigrams\n",
    "\n",
    "common_words_bi = get_top_n_bigram(df_clean.com_after_rem_pos, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xkh7vtbtJrF-"
   },
   "outputs": [],
   "source": [
    "#Plotting the top 10 words in the bigram frequency\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "df_bi = pd.DataFrame(common_words_bi[0:10], columns = ['bigram' , 'count'])\n",
    "fig = sns.barplot(y=df_bi['bigram'], x=df_bi['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "REcVxNfvJrF-"
   },
   "outputs": [],
   "source": [
    "#Write your code here to find the top 30 trigram frequency among the complaints in the cleaned datafram(df_clean). \n",
    "\n",
    "# function to find the top n trigrams\n",
    "\n",
    "def get_top_n_trigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(3, 3), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    \n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the top 10 words in the trigram frequency\n",
    "\n",
    "common_words_tri = get_top_n_trigram(df_clean.com_after_rem_pos, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the top-10 trigrams\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "df_tri = pd.DataFrame(common_words_tri[0:10], columns = ['trigram' , 'count'])\n",
    "fig = sns.barplot(y=df_tri['trigram'], x=df_tri['count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUXzFji0JrF_"
   },
   "source": [
    "## The personal details of customer has been masked in the dataset with xxxx. Let's remove the masked text as this will be of no use for our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wKda-a_IJrF_"
   },
   "outputs": [],
   "source": [
    "# remove 'xxxx'\n",
    "\n",
    "df_clean['com_after_rem_pos'] = df_clean['com_after_rem_pos'].str.replace('xxxx','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9UIFk8fQJrF_"
   },
   "outputs": [],
   "source": [
    "#All masked texts has been removed\n",
    "\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-I0k0QtJrGA"
   },
   "source": [
    "## Feature Extraction\n",
    "Convert the raw texts to a matrix of TF-IDF features\n",
    "\n",
    "**max_df** is used for removing terms that appear too frequently, also known as \"corpus-specific stop words\"\n",
    "max_df = 0.95 means \"ignore terms that appear in more than 95% of the complaints\"\n",
    "\n",
    "**min_df** is used for removing terms that appear too infrequently\n",
    "min_df = 2 means \"ignore terms that appear in less than 2 complaints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y8fGwaCPJrGA"
   },
   "outputs": [],
   "source": [
    "#Write your code here to initialise the TfidfVectorizer \n",
    "\n",
    "tfidf = TfidfVectorizer(max_df = 0.95, min_df = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYzD85nTJrGA"
   },
   "source": [
    "#### Create a document term matrix using fit_transform\n",
    "\n",
    "The contents of a document term matrix are tuples of (complaint_id,token_id) tf-idf score:\n",
    "The tuples that are not there have a tf-idf score of 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code here to create the Document Term Matrix by transforming the complaints column present in df_clean.\n",
    "\n",
    "dtm = tfidf.fit_transform(df_clean.com_after_rem_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtm.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature names using tfidf.get_feature_names()\n",
    "\n",
    "pd.DataFrame(dtm.toarray(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Q9lwvNEJrGB"
   },
   "source": [
    "## Topic Modelling using NMF\n",
    "\n",
    "Non-Negative Matrix Factorization (NMF) is an unsupervised technique so there are no labeling of topics that the model will be trained on. The way it works is that, NMF decomposes (or factorizes) high-dimensional vectors into a lower-dimensional representation. These lower-dimensional vectors are non-negative which also means their coefficients are non-negative.\n",
    "\n",
    "In this task you have to perform the following:\n",
    "\n",
    "* Find the best number of clusters \n",
    "* Apply the best number to create word clusters\n",
    "* Inspect & validate the correction of each cluster wrt the complaints \n",
    "* Correct the labels if needed \n",
    "* Map the clusters to topics/cluster names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "amLT4omWJrGB"
   },
   "outputs": [],
   "source": [
    "# import NMF library\n",
    "\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wYR1xUTJrGD"
   },
   "source": [
    "## Manual Topic Modeling\n",
    "You need to do take the trial & error approach to find the best num of topics for your NMF model.\n",
    "\n",
    "The only parameter that is required is the number of components i.e. the number of topics we want. This is the most crucial step in the whole topic modeling process and will greatly affect how good your final topics are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have tried the 8 and 10 topics as well and found out that 5 is the best num of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgd2A6bhJrGD"
   },
   "outputs": [],
   "source": [
    "#Load your nmf_model with the n_components i.e 5\n",
    "num_topics = 5 #write the value you want to test out\n",
    "\n",
    "#keep the random_state =40\n",
    "nmf_model = NMF(n_components=num_topics, random_state=40) #write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VPMDYbt_JrGE"
   },
   "outputs": [],
   "source": [
    "nmf_model.fit(dtm)\n",
    "\n",
    "print(len(tfidf.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document-topic matrix\n",
    "W1 = nmf_model.fit_transform(dtm)\n",
    "\n",
    "# Topic-term matrix\n",
    "H1 = nmf_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the Top15 words for each of the topics\n",
    "\n",
    "words = np.array(tfidf.get_feature_names())\n",
    "topic_words = pd.DataFrame(np.zeros((num_topics, 15)), index=[f'Topic {i + 1}' for i in range(num_topics)],\n",
    "                           columns=[f'Word {i + 1}' for i in range(15)]).astype(str)\n",
    "for i in range(num_topics):\n",
    "    ix = H1[i].argsort()[::-1][:15]\n",
    "    topic_words.iloc[i] = words[ix]\n",
    "\n",
    "topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H1 = pd.DataFrame(H1, index=['Topic 1', 'Topic 2', 'Topic 3', 'Topic 4', 'Topic 5'], columns=tfidf.get_feature_names())\n",
    "H1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top words for each topic\n",
    "\n",
    "fig, ax = plt.subplots(nrows=5, ncols=1, figsize=(15, 10))\n",
    "for i in range(5):\n",
    "    words = H1.loc[f'Topic {i + 1}'].sort_values(ascending=False)[:20]\n",
    "    words.plot(ax=ax[i], kind='bar', rot=0)\n",
    "    ax[i].set_title(f'Topic {i + 1}')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After evaluating the mapping, if the topics assigned are correct then assign these names to the relevant topic:\n",
    "* Bank Account services\n",
    "* Credit card or prepaid card\n",
    "* Theft/Dispute Reporting\n",
    "* Mortgage/Loan\n",
    "* Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After analysing the Topic 1-5 we have created the following topic mapping\n",
    "\n",
    "topic_mapping = {\n",
    "    'Topic 1': 'Other',\n",
    "    'Topic 2': 'Theft/Dispute reporting',\n",
    "    'Topic 3': 'Mortgages/loans',\n",
    "    'Topic 4': 'Credit card / Prepaid card',\n",
    "    'Topic 5': 'Bank account services'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall the document-topic matrix, W1\n",
    "#Assign the best topic to each of the cmplaints in Topic Column\n",
    "\n",
    "W1 = pd.DataFrame(W1, columns=[f'Topic {i + 1}' for i in range(num_topics)])\n",
    "W1['max_topic'] = W1.apply(lambda x: topic_mapping.get(x.idxmax()), axis=1)\n",
    "\n",
    "nmf_topics = W1[pd.notnull(W1['max_topic'])]\n",
    "nmf_topics.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "peyYv-ORJrGF"
   },
   "outputs": [],
   "source": [
    "#Add the column `Topic` column to df_clean\n",
    "\n",
    "df_clean['Topic'] = nmf_topics.max_topic #write your code to assign topics to each rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fLh_Gf3nJrGF"
   },
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify some complaints and corresponding topics\n",
    "\n",
    "for i in range(15000,15005):\n",
    "    print(df_clean.complaint[i])\n",
    "    print(\"Topic:\", df_clean.Topic[i])\n",
    "    print(\"--\"*65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After verifying the above 25-30 complaints and repective topics manually, we have found that the topics and the complaints are matching mostly and there are very few mismatches. This is the best topic-complaint mapping we have found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Mu0QBOcJrGH"
   },
   "source": [
    "## Supervised model to predict any new complaints to the relevant Topics.\n",
    "\n",
    "You have now build the model to create the topics for each complaints. Now in the below section you will use them to classify any new complaints.\n",
    "\n",
    "Since you will be using supervised learning technique we have to convert the topic names to numbers(numpy arrays only understand numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xx-FrbkWJrGH"
   },
   "outputs": [],
   "source": [
    "#Keep the columns\"complaint_what_happened\" & \"Topic\" only in the new dataframe --> training_data\n",
    "\n",
    "training_data = df_clean[['complaint', 'Topic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVg2pa12JrGI"
   },
   "outputs": [],
   "source": [
    "# training data\n",
    "\n",
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "280Vbqk-7a8M"
   },
   "source": [
    "#### Apply the supervised models on the training data created. In this process, you have to do the following:\n",
    "\n",
    "* Create the vector counts using Count Vectoriser\n",
    "* Transform the word vecotr to tf-idf\n",
    "* Create the train & test data using the train_test_split on the tf-idf & topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# Create the vector counts using Count Vectoriser\n",
    "count_vect = CountVectorizer(max_df = 0.95, min_df = 2)\n",
    "\n",
    "# Transform the word vecotr to tf-idf. we will use TfidfTransformer()\n",
    "\n",
    "vectorized_complaint = count_vect.fit_transform(df_clean.complaint)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_model = tfidf_transformer.fit_transform(vectorized_complaint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the target variable to numeric value. topics will be assigned value 0-4 as below.\n",
    "\n",
    "df_clean['Topic'] = df_clean['Topic'].map({\n",
    "    'Other': 0,\n",
    "    'Theft/Dispute reporting': 1, \n",
    "    'Mortgages/loans': 2, \n",
    "    'Credit card / Prepaid card': 3,\n",
    "    'Bank account services': 4\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create X and y variable\n",
    "\n",
    "X = pd.DataFrame(tfidf_model.toarray(), columns=count_vect.get_feature_names())\n",
    "\n",
    "y = df_clean['Topic']\n",
    "\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value counts for each class\n",
    "\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# will split the 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size =0.3, random_state=100)\n",
    "\n",
    "print(\"train shape:\", X_train.shape)\n",
    "print(\"test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMU3vj6w-wqL"
   },
   "source": [
    "You have to try atleast 2 models on the train & test data from these options:\n",
    "* Logistic regression\n",
    "* Decision Tree\n",
    "* Random Forest\n",
    "* Naive Bayes (optional)\n",
    "\n",
    "**Using the required evaluation metrics judge the tried models and select the ones performing the best**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models:\n",
    "- Create a function to calculate the accuracy score and print the confusion matrix \n",
    "- We have decided to create below models:\n",
    "    1. Naive bayes (Bernoulli and Mutinomial)\n",
    "    2. Random forest\n",
    "    2. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import confusion matrix and accuracy score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# function to print the accuracy and the confusion matrix\n",
    "def get_metrics(y_train, y_pred_train, y_test, y_pred_test):\n",
    "    print(\"Train Accuracy :\", accuracy_score(y_true=y_train, y_pred = y_pred_train))\n",
    "    print(\"Train Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_train, y_pred_train))\n",
    "    print(\"-\"*50)\n",
    "    print(\"Test Accuracy :\", accuracy_score(y_true=y_test, y_pred = y_pred_test))\n",
    "    print(\"Test Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the Bernoulli Naive Bayes model \n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "bnb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting and getting the metrics for the train and test data\n",
    "y_pred_train = bnb.predict(X_train)\n",
    "y_pred_test = bnb.predict(X_test)\n",
    "\n",
    "get_metrics(y_train, y_pred_train, y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the Multinomial Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb=MultinomialNB()\n",
    "mnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predicting and getting the metrics for the train and test data\n",
    "y_pred_train = mnb.predict(X_train)\n",
    "y_pred_test = mnb.predict(X_test)\n",
    "\n",
    "get_metrics(y_train, y_pred_train, y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forest\n",
    "- We have tuned the hyper-parameters and using the best parameters here to build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "udLHpPsZJrGI"
   },
   "outputs": [],
   "source": [
    "# training the Random Forest model\n",
    "# We have tuned the hyper-parameters and using the best parameters here to build the model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42, \n",
    "                                  n_jobs=-1, \n",
    "                                  max_depth =100, \n",
    "                                  min_samples_leaf = 150, \n",
    "                                  max_features = 1000,\n",
    "                                  n_estimators = 300)\n",
    "\n",
    "rf_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting and getting the metrics for the train and test data\n",
    "y_pred_train = rf_model.predict(X_train)\n",
    "y_pred_test = rf_model.predict(X_test)\n",
    "\n",
    "get_metrics(y_train, y_pred_train, y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the Logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lm = LogisticRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting and getting the metrics for the train and test data\n",
    "y_pred_train = lm.predict(X_train)\n",
    "y_pred_test = lm.predict(X_test)\n",
    "\n",
    "get_metrics(y_train, y_pred_train, y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "We have the below four models.\n",
    "1. Bernoulli Naive Bayes - 64% Test accuracy\n",
    "2. Multinomial Naive Bayes - 72.7% Test accuracy\n",
    "3. Random Forest - 82.2% Test accuracy\n",
    "4. Logistic Regression - 93% Test accuracy\n",
    "\n",
    "#### Among all these Logistic regression is the best model as per the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some custom text\n",
    "custom_text = \"Applicable loan admin charge fee waiver is not applied on my loan account number xxxx-xxxxxx\"\n",
    "\n",
    "vectorized = tfidf_transformer.transform(count_vect.transform([custom_text]))\n",
    "cust_x = pd.DataFrame(vectorized.toarray(), columns=count_vect.get_feature_names())\n",
    "\n",
    "# make prediction\n",
    "predictions = lm.predict(cust_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topics label and the topics are as below \n",
    "\n",
    "0. Other\n",
    "1. Theft/Dispute reporting\n",
    "2. Mortgages/loans\n",
    "3. Credit card / Prepaid card\n",
    "4. Bank account services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The final model (Logistic Regression) is able to correctly predict custom text as `Mortgages/loans`"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "T9jD_6SeJrF3",
    "5DfCSbbmJrF4",
    "yYzD85nTJrGA",
    "piyLxzj6v07j",
    "280Vbqk-7a8M"
   ],
   "name": "Automatic_Ticket_Classification_Starter_code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
